{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import xmltodict\n",
    "\n",
    "from typing import List, Set, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 8] nodename nor servname provided, or not\n",
      "[nltk_data]     known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "# Ensure that necessary nltk resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_name(names):\n",
    "    if isinstance(names, list):\n",
    "        for name in names:\n",
    "            if name.get('@type') == 'primary':\n",
    "                return name.get('@value')\n",
    "    elif isinstance(names, dict) and names.get('@type') == 'primary':\n",
    "        return names.get('@value')\n",
    "    return None\n",
    "\n",
    "def retrieve_comments(index):\n",
    "    URL = \"https://boardgamegeek.com/xmlapi2/thing?id=\" + str(index) + \"&type=boardgame&comments=1\"\n",
    "    response = requests.get(URL)\n",
    "    data = xmltodict.parse(response.content)\n",
    "    game_name = get_primary_name(data[\"items\"][\"item\"]['name'])\n",
    "\n",
    "    comments = []\n",
    "    if 'items' in data and 'item' in data['items']:\n",
    "        item = data['items']['item']\n",
    "        if 'comments' in item and 'comment' in item['comments']:\n",
    "            for comment in item['comments']['comment']:\n",
    "                comments.append({game_name: comment['@value']})\n",
    "\n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Metal Planète</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Box has shelfwear, contents in great condition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Excellent mechanics, but the gameplay may be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exciting and tense game so far out of print yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I made a homemade copy. Still working on finis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 exemplaires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>+ expansion!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>An almost purely tactical game with very littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>This is certainly one of the best game I playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A very good strategy game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>/*#L4*//*stay*/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Full Metal Planète\n",
       "0   Box has shelfwear, contents in great condition...\n",
       "1   Excellent mechanics, but the gameplay may be a...\n",
       "2   Exciting and tense game so far out of print yo...\n",
       "3   I made a homemade copy. Still working on finis...\n",
       "4                                       2 exemplaires\n",
       "..                                                ...\n",
       "95                                       + expansion!\n",
       "96  An almost purely tactical game with very littl...\n",
       "97  This is certainly one of the best game I playe...\n",
       "98                          A very good strategy game\n",
       "99                                    /*#L4*//*stay*/\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_comments(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event concept extraction algorithm\n",
    "----\n",
    "\n",
    "The algorithm for capturing event concepts matches object concepts with normalized verb chunks. This is achieved by utilizing a parse graph that maps all the multi-word expressions contained in the knowledge bases.\n",
    "1.\tMatch Object and Verb Phrases:\n",
    "\t- The algorithm searches for matches between the object concepts and the normalized verb phrases.\n",
    "2.\tUtilize a Parse Graph:\n",
    "\t- A directed, unweighted parse graph is used to quickly detect multi-word concepts without performing an exhaustive search through all possible word combinations that can form a commonsense concept.\n",
    "3.\tRemove Redundant Terms:\n",
    "\t- Single-word concepts, such as “house,” that already appear in the clause as part of a multi-word concept, like “beautiful house,” are considered pleonastic (providing redundant information) and are discarded.\n",
    "4.\tExtract Event Concepts:\n",
    "\t- The algorithm extracts event concepts such as “go market,” “buy some fruits,” “buy fruits,” and “buy vegetables.”\n",
    "\t- These event concepts represent Script-Based Object Concepts (SBoCs) and can be fed into a commonsense reasoning algorithm for further processing.\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_concept_extraction(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: extracted event concepts\n",
    "    \"\"\"\n",
    "    concepts: Set[str] = set()  # initialize an empty set to store unique concepts\n",
    "    doc = nlp(sentence)  # process the sentence with spaCy\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # identify all verbs in the sentence\n",
    "        verbs = [token for token in sent if token.pos_ == 'VERB']\n",
    "        # identify all noun phrases in the sentence\n",
    "        noun_phrases = list(sent.noun_chunks)\n",
    "\n",
    "        for verb in verbs:\n",
    "            # stem the verb\n",
    "            stemmed_verb = verb.lemma_\n",
    "            # find noun phrases associated with the verb\n",
    "            associated_nouns = [np for np in noun_phrases if np.root.head == verb]\n",
    "\n",
    "            for np in associated_nouns:\n",
    "                # extract adjectives in the noun phrase\n",
    "                adjectives = [token.text for token in np if token.pos_ == 'ADJ']\n",
    "                if len(np) > 1:\n",
    "                    # if the noun phrase contains more than one word, form a concept with the verb and noun phrase\n",
    "                    concept = f\"{stemmed_verb} {' '.join([token.text for token in np])}\"\n",
    "                    if adjectives:\n",
    "                        concept += f\" {' '.join(adjectives)}\"\n",
    "                    concepts.add(concept)\n",
    "                else:\n",
    "                    # handle single-word noun phrases\n",
    "                    single_word_concept = np.text\n",
    "                    if not any(single_word_concept in concept for concept in concepts):\n",
    "                        concept = f\"{stemmed_verb} {single_word_concept}\"\n",
    "                        if adjectives:\n",
    "                            concept += f\" {' '.join(adjectives)}\"\n",
    "                        concepts.add(concept)\n",
    "\n",
    "        for np in noun_phrases:\n",
    "            # handle noun phrases associated with auxiliary verbs\n",
    "            if np.root.head.pos_ == 'AUX':\n",
    "                adjectives = [token.text for token in np if token.pos_ == 'ADJ']\n",
    "                concept = f\"be {' '.join([token.text for token in np])}\"\n",
    "                if adjectives:\n",
    "                    concept += f\" {' '.join(adjectives)}\"\n",
    "                concepts.add(concept)\n",
    "\n",
    "    return list(concepts)\n",
    "\n",
    "def extract_concepts_from_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a given text by splitting it into sentences first.\n",
    "\n",
    "    Args:\n",
    "        text (str): text to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: extracted event concepts\n",
    "    \"\"\"\n",
    "    all_concepts = set()\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        concepts = event_concept_extraction(sent.text)\n",
    "        all_concepts.update(concepts)\n",
    "    return list(all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comment:\n",
      "\n",
      "\"Like it.  Lightweight.  Scoring requires some calculation.\"\n",
      "\n",
      "Extracted concepts:  ['require Scoring', 'require some calculation']\n"
     ]
    }
   ],
   "source": [
    "sentence = str(retrieve_comments(50).iloc[0, 0])\n",
    "print(\"Original comment:\\n\")\n",
    "print(f\"\"\"\\\"{sentence}\\\"\"\"\")\n",
    "concepts = extract_concepts_from_text(sentence)\n",
    "print(\"\\nExtracted concepts: \", concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ')\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def get_combined_embedding(words, embeddings):\n",
    "    valid_embeddings = [embeddings.get(word) for word in words if word in embeddings]\n",
    "    if not valid_embeddings:\n",
    "        return None\n",
    "    return np.mean(valid_embeddings, axis=0)\n",
    "\n",
    "embeddings = load_embeddings('./data/numberbatch-en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings = []\n",
    "valid_concepts = []\n",
    "for concept in concepts:\n",
    "    words = concept.split()\n",
    "    embedding = get_combined_embedding(words, embeddings)\n",
    "    if embedding is not None:\n",
    "        concept_embeddings.append(embedding)\n",
    "        valid_concepts.append(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5 if len(concepts) > 5 else len(concepts)\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=num_clusters, random_state=0).fit(concept_embeddings)\n",
    "labels = kmedoids.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_concepts = {}\n",
    "for label, concept in zip(labels, valid_concepts):\n",
    "    if label not in clustered_concepts:\n",
    "        clustered_concepts[label] = []\n",
    "    clustered_concepts[label].append((concept, concept_embeddings[valid_concepts.index(concept)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_concepts = []\n",
    "for label, items in clustered_concepts.items():\n",
    "    medoid_index = kmedoids.medoid_indices_[label]\n",
    "    reduced_concepts.append(valid_concepts[medoid_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Concepts:\n",
      "require Scoring\n",
      "require some calculation\n"
     ]
    }
   ],
   "source": [
    "print(\"Reduced Concepts:\")\n",
    "for concept in reduced_concepts:\n",
    "    print(concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple matching for classification\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified classes\n",
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_concepts(concepts: List[str], class_keywords: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Classifies extracted concepts into predefined categories based on keywords.\n",
    "\n",
    "    Args:\n",
    "        concepts (List[str]): The list of extracted concepts.\n",
    "        class_keywords (Dict[str, List[str]]): A dictionary where keys are class names and values are lists of keywords.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Set[str]]: A dictionary where keys are class names and values are sets of concepts that match the keywords.\n",
    "    \"\"\"\n",
    "    classified_concepts: Dict[str, Set[str]] = {category: set() for category in class_keywords}\n",
    "\n",
    "    for concept in concepts:\n",
    "        for category, keywords in class_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in concept:\n",
    "                    classified_concepts[category].add(concept)\n",
    "\n",
    "    return classified_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified concepts:  {'luck or alea': set(), 'bookkeeping': set(), 'downtime': set(), 'interaction': set(), 'bash the leader': set(), 'complicated': set(), 'complex': set()}\n"
     ]
    }
   ],
   "source": [
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}\n",
    "\n",
    "classified_concepts = classify_concepts(concepts, class_keywords)\n",
    "print(\"\\nClassified concepts: \", classified_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use of similarity meassure\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_similarity(concept, class_keywords, embeddings, similarity_threshold=0.5, top_n_classes=3):\n",
    "    concept_words = concept.split()\n",
    "    concept_embedding = get_combined_embedding(concept_words, embeddings)\n",
    "    if concept_embedding is None:\n",
    "        return None, None\n",
    "    \n",
    "    similarities = []\n",
    "    for class_name, keywords in class_keywords.items():\n",
    "        class_embedding = get_combined_embedding(keywords, embeddings)\n",
    "        if class_embedding is None:\n",
    "            continue\n",
    "        similarity = cosine_similarity(concept_embedding, class_embedding)\n",
    "        if similarity is not None:\n",
    "            similarities.append((class_name, similarity))\n",
    "    \n",
    "    # Filter classes based on similarity threshold\n",
    "    similarities = [item for item in similarities if item[1] >= similarity_threshold]\n",
    "    # Sort by similarity and get top N\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_classes = similarities[:top_n_classes]\n",
    "    \n",
    "    if not top_classes:\n",
    "        return None, None\n",
    "            \n",
    "    return top_classes[0]  # Return the best class and its similarity\n",
    "\n",
    "# compute the similarity\n",
    "assignments = {}\n",
    "for concept in concepts:\n",
    "    best_class, similarity = compute_similarity(concept, class_keywords, embeddings, similarity_threshold=0.1, top_n_classes=1)\n",
    "    assignments[concept] = (best_class, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignments:\n",
      "Concept 'require Scoring' is assigned to class 'complicated' with similarity 0.32\n",
      "Concept 'require some calculation' is assigned to class 'complicated' with similarity 0.35\n"
     ]
    }
   ],
   "source": [
    "# ouput of the resutls\n",
    "print(\"Assignments:\")\n",
    "for concept, best_class in assignments.items():\n",
    "    if best_class != (None, None):\n",
    "        class_name, similarity = best_class\n",
    "        print(f\"Concept '{concept}' is assigned to class '{class_name}' with similarity {similarity:.2f}\")\n",
    "    else:\n",
    "        print(f\"Concept '{concept}' could not be assigned to any class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
