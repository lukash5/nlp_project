{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import xmltodict\n",
    "\n",
    "from typing import Union, List, Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure that necessary nltk resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_name(names: Union[List[Dict[str, str]], Dict[str, str]]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the primary name from the given names and formats it to lowercase with underscores.\n",
    "\n",
    "    Args:\n",
    "        names (Union[List[Dict[str, str]], Dict[str, str]]): The list or dict containing name information.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The formatted primary name, or None if not found.\n",
    "    \"\"\"\n",
    "    if isinstance(names, list):\n",
    "        for name in names:\n",
    "            if name.get('@type') == 'primary':\n",
    "                primary_name = name.get('@value')\n",
    "                return primary_name.lower().replace(' ', '_')\n",
    "    elif isinstance(names, dict) and names.get('@type') == 'primary':\n",
    "        primary_name = names.get('@value')\n",
    "        return primary_name.lower().replace(' ', '_')\n",
    "    return None\n",
    "\n",
    "def retrieve_comments(index: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves comments for the board game with the given index from the BoardGameGeek API and formats the game name.\n",
    "\n",
    "    Args:\n",
    "        index (int): The index of the board game.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the comments with the formatted game name as keys.\n",
    "    \"\"\"\n",
    "    URL = f\"https://boardgamegeek.com/xmlapi2/thing?id={index}&type=boardgame&comments=1\"\n",
    "    response = requests.get(URL)\n",
    "    data = xmltodict.parse(response.content)\n",
    "    \n",
    "    game_name = get_primary_name(data[\"items\"][\"item\"]['name'])\n",
    "\n",
    "    comments = []\n",
    "    if 'items' in data and 'item' in data['items']:\n",
    "        item = data['items']['item']\n",
    "        if 'comments' in item and 'comment' in item['comments']:\n",
    "            for comment in item['comments']['comment']:\n",
    "                comments.append({game_name: comment['@value']})\n",
    "\n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_metal_planète</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Box has shelfwear, contents in great condition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Excellent mechanics, but the gameplay may be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exciting and tense game so far out of print yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I made a homemade copy. Still working on finis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 exemplaires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>+ expansion!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>An almost purely tactical game with very littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>This is certainly one of the best game I playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A very good strategy game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>/*#L4*//*stay*/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   full_metal_planète\n",
       "0   Box has shelfwear, contents in great condition...\n",
       "1   Excellent mechanics, but the gameplay may be a...\n",
       "2   Exciting and tense game so far out of print yo...\n",
       "3   I made a homemade copy. Still working on finis...\n",
       "4                                       2 exemplaires\n",
       "..                                                ...\n",
       "95                                       + expansion!\n",
       "96  An almost purely tactical game with very littl...\n",
       "97  This is certainly one of the best game I playe...\n",
       "98                          A very good strategy game\n",
       "99                                    /*#L4*//*stay*/\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_comments(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event concept extraction algorithm\n",
    "----\n",
    "\n",
    "The algorithm for capturing event concepts matches object concepts with normalized verb chunks. This is achieved by utilizing a parse graph that maps all the multi-word expressions contained in the knowledge bases.\n",
    "1.\tMatch Object and Verb Phrases:\n",
    "\t- The algorithm searches for matches between the object concepts and the normalized verb phrases.\n",
    "2.\tUtilize a Parse Graph:\n",
    "\t- A directed, unweighted parse graph is used to quickly detect multi-word concepts without performing an exhaustive search through all possible word combinations that can form a commonsense concept.\n",
    "3.\tRemove Redundant Terms:\n",
    "\t- Single-word concepts, such as “house,” that already appear in the clause as part of a multi-word concept, like “beautiful house,” are considered pleonastic (providing redundant information) and are discarded.\n",
    "4.\tExtract Event Concepts:\n",
    "\t- The algorithm extracts event concepts such as “go market,” “buy some fruits,” “buy fruits,” and “buy vegetables.”\n",
    "\t- These event concepts represent Script-Based Object Concepts (SBoCs) and can be fed into a commonsense reasoning algorithm for further processing.\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_concept_extraction(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: extracted event concepts\n",
    "    \"\"\"\n",
    "    concepts: Set[str] = set()  # initialize an empty set to store unique concepts\n",
    "    doc = nlp(sentence)  # process the sentence with spaCy\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # identify all verbs in the sentence\n",
    "        verbs = [token for token in sent if token.pos_ == 'VERB']\n",
    "        # identify all noun phrases in the sentence\n",
    "        noun_phrases = list(sent.noun_chunks)\n",
    "\n",
    "        for verb in verbs:\n",
    "            # stem the verb\n",
    "            stemmed_verb = verb.lemma_\n",
    "            # find noun phrases associated with the verb\n",
    "            associated_nouns = [np for np in noun_phrases if np.root.head == verb]\n",
    "\n",
    "            for np in associated_nouns:\n",
    "                # extract adjectives in the noun phrase\n",
    "                adjectives = [token.text for token in np if token.pos_ == 'ADJ']\n",
    "                if len(np) > 1:\n",
    "                    # if the noun phrase contains more than one word, form a concept with the verb and noun phrase\n",
    "                    concept = f\"{stemmed_verb} {' '.join([token.text for token in np])}\"\n",
    "                    if adjectives:\n",
    "                        concept += f\" {' '.join(adjectives)}\"\n",
    "                    concepts.add(concept)\n",
    "                else:\n",
    "                    # handle single-word noun phrases\n",
    "                    single_word_concept = np.text\n",
    "                    if not any(single_word_concept in concept for concept in concepts):\n",
    "                        concept = f\"{stemmed_verb} {single_word_concept}\"\n",
    "                        if adjectives:\n",
    "                            concept += f\" {' '.join(adjectives)}\"\n",
    "                        concepts.add(concept)\n",
    "\n",
    "        for np in noun_phrases:\n",
    "            # handle noun phrases associated with auxiliary verbs\n",
    "            if np.root.head.pos_ == 'AUX':\n",
    "                adjectives = [token.text for token in np if token.pos_ == 'ADJ']\n",
    "                concept = f\"be {' '.join([token.text for token in np])}\"\n",
    "                if adjectives:\n",
    "                    concept += f\" {' '.join(adjectives)}\"\n",
    "                concepts.add(concept)\n",
    "\n",
    "    return list(concepts)\n",
    "\n",
    "def extract_concepts_from_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a given text by splitting it into sentences first.\n",
    "\n",
    "    Args:\n",
    "        text (str): text to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: extracted event concepts\n",
    "    \"\"\"\n",
    "    all_concepts = set()\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        concepts = event_concept_extraction(sent.text)\n",
    "        all_concepts.update(concepts)\n",
    "    return list(all_concepts)\n",
    "\n",
    "def extract_concepts_from_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a pandas Series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): Series to process\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Series with extracted event concepts\n",
    "    \"\"\"\n",
    "    all_concepts_series = series.apply(lambda text: extract_concepts_from_text(str(text)))\n",
    "    return all_concepts_series\n",
    "\n",
    "def extract_concepts_from_dataframe(df: pd.DataFrame, text_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from specified columns of a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to process\n",
    "        text_columns (List[str]): List of column names containing text data to process\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with extracted event concepts in corresponding new columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    for column in text_columns:\n",
    "        result_df[f\"{column}_concepts\"] = extract_concepts_from_series(df[column])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare comments for a board game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ricochet_robots</th>\n",
       "      <th>ricochet_robots_concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very interesting multi-player puzzle game. The...</td>\n",
       "      <td>[be my feeling, get the game, change every tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's a brillant game that can be played by a t...</td>\n",
       "      <td>[be It, play that, deserve it, be a brillant g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mind consuming game. A party game just because...</td>\n",
       "      <td>[play you, play it, play A party game]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is a wonderful puzzle that is great fun in ...</td>\n",
       "      <td>[be that, be great fun great, be It, be a wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brain burner! Easy play but very challenging.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Makes your brain melt. Best played without 'th...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reviewed on my blog.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Not for everyone, but I love it, particularly ...</td>\n",
       "      <td>[frustrate the others, have an even group even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ricochet Robot is a great brain burner, but it...</td>\n",
       "      <td>[be Ricochet Robot, guess I, love who, have go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>played</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This game is probably like my personal torture...</td>\n",
       "      <td>[beat he, be This game, own it, see enough ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>130929 Its not in my wheelhouse, a bit too abs...</td>\n",
       "      <td>[love I, love it, be this, be an excellent gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A great little puzzle game that I'm still comi...</td>\n",
       "      <td>[come I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Donation from Rio Grande Games - March 2011</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8436017221602 [Estante 08] [M000153]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ricochet_robots  \\\n",
       "0   Very interesting multi-player puzzle game. The...   \n",
       "1   It's a brillant game that can be played by a t...   \n",
       "2   mind consuming game. A party game just because...   \n",
       "3   It is a wonderful puzzle that is great fun in ...   \n",
       "4       Brain burner! Easy play but very challenging.   \n",
       "5   Makes your brain melt. Best played without 'th...   \n",
       "6                                reviewed on my blog.   \n",
       "7   Not for everyone, but I love it, particularly ...   \n",
       "8   Ricochet Robot is a great brain burner, but it...   \n",
       "9                                              played   \n",
       "10  This game is probably like my personal torture...   \n",
       "11  130929 Its not in my wheelhouse, a bit too abs...   \n",
       "12  A great little puzzle game that I'm still comi...   \n",
       "13        Donation from Rio Grande Games - March 2011   \n",
       "14               8436017221602 [Estante 08] [M000153]   \n",
       "\n",
       "                             ricochet_robots_concepts  \n",
       "0   [be my feeling, get the game, change every tur...  \n",
       "1   [be It, play that, deserve it, be a brillant g...  \n",
       "2              [play you, play it, play A party game]  \n",
       "3   [be that, be great fun great, be It, be a wond...  \n",
       "4                                                  []  \n",
       "5                                                  []  \n",
       "6                                                  []  \n",
       "7   [frustrate the others, have an even group even...  \n",
       "8   [be Ricochet Robot, guess I, love who, have go...  \n",
       "9                                                  []  \n",
       "10  [beat he, be This game, own it, see enough ste...  \n",
       "11  [love I, love it, be this, be an excellent gam...  \n",
       "12                                           [come I]  \n",
       "13                                                 []  \n",
       "14                                                 []  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = retrieve_comments(51)\n",
    "concepts = extract_concepts_from_dataframe(comments, [comments.columns[0]])\n",
    "concepts.to_csv(f\"./{concepts.columns[0]}.csv\")\n",
    "concepts.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads word embeddings from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: Dictionary of word embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ')\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def get_combined_embedding(words: List[str], embeddings: Dict[str, np.ndarray]) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes the combined embedding for a list of words.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): List of words.\n",
    "        embeddings (Dict[str, np.ndarray]): Dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Optional[np.ndarray]: Combined embedding vector, or None if no valid embeddings found.\n",
    "    \"\"\"\n",
    "    valid_embeddings = [embeddings.get(word) for word in words if word in embeddings]\n",
    "    if not valid_embeddings:\n",
    "        return None\n",
    "    return np.mean(valid_embeddings, axis=0)\n",
    "\n",
    "\n",
    "def process_concepts_from_dataframe(df: pd.DataFrame, column_name: str, embeddings: Dict[str, np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes concepts from a DataFrame column and computes their embeddings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing original sentences and concepts.\n",
    "        embeddings (Dict[str, np.ndarray]): Dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with original sentences, concepts, and their embeddings.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row[column_name]\n",
    "        concepts = row[column_name + '_concepts']\n",
    "        concept_embedding = []\n",
    "        num_concepts = 0\n",
    "        for concept in concepts:\n",
    "            words = concept.split()\n",
    "            embedding = get_combined_embedding(words, embeddings)\n",
    "\n",
    "            if embedding is not None:\n",
    "                num_concepts += 1\n",
    "                concept_embedding.append(embedding)\n",
    "\n",
    "        processed_data.append({\n",
    "            'sentence': sentence,\n",
    "            'concept': concepts,\n",
    "            'embedding': concept_embedding,\n",
    "            'num_concepts': num_concepts,\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(processed_data)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings = load_embeddings('./data/numberbatch-en.txt')\n",
    "\n",
    "# Process concepts and compute embeddings\n",
    "concepts = process_concepts_from_dataframe(concepts, concepts.columns[0], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukas/anaconda3/envs/nlp/lib/python3.12/site-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 1 is empty! self.labels_[self.medoid_indices_[1]] may not be labeled with its corresponding cluster (1).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def cluster_concepts(concepts: pd.DataFrame, num_clusters: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clusters each concept using KMedoids separately and adds clustering results to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        concepts (pd.DataFrame): DataFrame containing concepts and their embeddings.\n",
    "        num_clusters (int): number of clusters\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with clustering results.\n",
    "    \"\"\"\n",
    "    clustered_concepts_list = []\n",
    "    \n",
    "    for index, row in concepts.iterrows():\n",
    "        embedding = row['embedding']\n",
    "        concepts = row['concept']\n",
    "        num_concepts = row['num_concepts']\n",
    "\n",
    "        reduced_concepts = []\n",
    "\n",
    "        if num_concepts != 0:\n",
    "            # Perform KMedoids clustering on this set\n",
    "            kmedoids = KMedoids(n_clusters=min(num_clusters, num_concepts), random_state=0).fit(embedding)\n",
    "            labels = kmedoids.labels_\n",
    "\n",
    "            clustered_concepts = {}\n",
    "            for label, concept in zip(labels, concepts):\n",
    "                if label not in clustered_concepts:\n",
    "                    clustered_concepts[label] = []\n",
    "                clustered_concepts[label].append((concept, embedding[concepts.index(concept)]))\n",
    "\n",
    "            for label, items in clustered_concepts.items():\n",
    "                medoid_index = kmedoids.medoid_indices_[label]\n",
    "                reduced_concepts.append(concepts[medoid_index])\n",
    "            \n",
    "        # Add clustering results to a new DataFrame\n",
    "        clustered_concepts_list.append({\n",
    "            'concept': concepts,\n",
    "            'clustered_concepts': reduced_concepts,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(clustered_concepts_list)\n",
    "\n",
    "# Cluster the concepts and get the reduced concepts\n",
    "reduced_concepts = cluster_concepts(concepts, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the pre-defined classes with keywords\n",
    "\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified classes\n",
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use of similarity meassure\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> Optional[float]:\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_similarity(concept: str, class_keywords: Dict[str, List[str]], embeddings: Dict[str, np.ndarray], \n",
    "                       similarity_threshold: float = 0.5, top_n_classes: int = 3) -> Optional[Tuple[str, float]]:\n",
    "    \n",
    "    concept_embedding = get_combined_embedding(concept.split(), embeddings)\n",
    "    if concept_embedding is None:\n",
    "        return None\n",
    "    \n",
    "    similarities = []\n",
    "    for class_name, keywords in class_keywords.items():\n",
    "        class_embedding = get_combined_embedding(keywords, embeddings)\n",
    "        if class_embedding is None:\n",
    "            continue\n",
    "        similarity = cosine_similarity(concept_embedding, class_embedding)\n",
    "        if similarity is not None:\n",
    "            similarities.append((class_name, similarity))\n",
    "    \n",
    "    # Filter classes based on similarity threshold\n",
    "    similarities = [item for item in similarities if item[1] >= similarity_threshold]\n",
    "    # Sort by similarity and get top N\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_classes = similarities[:top_n_classes]\n",
    "    \n",
    "    if not top_classes:\n",
    "        return None\n",
    "            \n",
    "    return top_classes[0]  # Return the best class and its similarity\n",
    "\n",
    "def assign_concepts_to_classes(concepts: pd.DataFrame, class_keywords: Dict[str, List[str]], embeddings: Dict[str, np.ndarray], \n",
    "                               similarity_threshold: float = 0.1, top_n_classes: int = 1) -> pd.DataFrame:\n",
    "    assignment_results = []\n",
    "    for _, row in concepts.iterrows():\n",
    "        concepts = row['clustered_concepts']\n",
    "        assignments = {}\n",
    "        for concept in concepts:\n",
    "            best_class = compute_similarity(concept, class_keywords, embeddings, similarity_threshold, top_n_classes)\n",
    "            assignments[concept] = best_class\n",
    "        \n",
    "        assignment_results.append({\n",
    "            'classified_concepts': assignments \n",
    "        })\n",
    "     \n",
    "    assignment_df = pd.DataFrame(assignment_results)\n",
    "    return assignment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign concepts to classes\n",
    "assigned_classes = assign_concepts_to_classes(reduced_concepts, class_keywords, embeddings)\n",
    "\n",
    "# Output the results\n",
    "assigned_classes.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
