{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "from typing import List, Set, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure that necessary nltk resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_name(names):\n",
    "    if isinstance(names, list):\n",
    "        for name in names:\n",
    "            if name.get('@type') == 'primary':\n",
    "                return name.get('@value')\n",
    "    elif isinstance(names, dict) and names.get('@type') == 'primary':\n",
    "        return names.get('@value')\n",
    "    return None\n",
    "\n",
    "def retrieve_comments(index):\n",
    "    URL = \"https://boardgamegeek.com/xmlapi2/thing?id=\" + str(index) + \"&type=boardgame&comments=1\"\n",
    "    response = requests.get(URL)\n",
    "    data = xmltodict.parse(response.content)\n",
    "    game_name = get_primary_name(data[\"items\"][\"item\"]['name'])\n",
    "\n",
    "    comments = []\n",
    "    if 'items' in data and 'item' in data['items']:\n",
    "        item = data['items']['item']\n",
    "        if 'comments' in item and 'comment' in item['comments']:\n",
    "            for comment in item['comments']['comment']:\n",
    "                comments.append({game_name: comment['@value']})\n",
    "\n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map Treebank POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_nouns(phrase):\n",
    "    tokens = word_tokenize(phrase)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    nouns = []\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        if wordnet_pos == wn.NOUN:\n",
    "            nouns.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "def extract_verbs_and_nouns(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    verbs = []\n",
    "    nouns = []\n",
    "    \n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        if wordnet_pos == wn.VERB:\n",
    "            verbs.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "        elif wordnet_pos == wn.NOUN:\n",
    "            nouns.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    \n",
    "    return verbs, nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_actions = ['use', 'enjoy', 'play', 'do', 'know', 'suspect', 'drop']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def find_possible_forms_of_objects(nouns):\n",
    "    \"\"\"Finds possible forms of object concepts from nouns.\"\"\"\n",
    "    forms = set()\n",
    "    for noun in nouns:\n",
    "        # Stemming to find the base form of the noun\n",
    "        base_form = stemmer.stem(noun)\n",
    "        forms.add(base_form)\n",
    "        \n",
    "        # Add singular and plural forms using WordNet\n",
    "        synsets = wn.synsets(noun, pos=wn.NOUN)\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                forms.add(lemma.name())\n",
    "                # Adding plural form\n",
    "                plural_form = lemma.name() + 's'\n",
    "                forms.add(plural_form)\n",
    "    \n",
    "    return forms\n",
    "\n",
    "def property_matches(concept):\n",
    "    \"\"\"Retrieve property matches for a concept from a knowledge base.\n",
    "    Uses WordNet to find related properties and synonyms.\n",
    "    \"\"\"\n",
    "    properties = set()\n",
    "    synsets = wn.synsets(concept, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        # Add the concept itself\n",
    "        properties.add(concept)\n",
    "        \n",
    "        # Add synonyms\n",
    "        for lemma in synset.lemmas():\n",
    "            properties.add(lemma.name())\n",
    "        \n",
    "        # Add hypernyms (more general concepts)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            properties.add(hypernym.name().split('.')[0])\n",
    "        \n",
    "        # Add hyponyms (more specific concepts)\n",
    "        for hyponym in synset.hyponyms():\n",
    "            properties.add(hyponym.name().split('.')[0])\n",
    "    \n",
    "    return properties\n",
    "\n",
    "def link_objects_to_verb(verb, objects):\n",
    "    \"\"\"Links objects to a stemmed verb to form event concepts.\"\"\"\n",
    "    events = []\n",
    "    for obj in objects:\n",
    "        events.append(f\"{verb} {obj}\")\n",
    "    return events\n",
    "\n",
    "def extract_event_concepts(sentence):\n",
    "    verbs, nouns = extract_verbs_and_nouns(sentence)\n",
    "    \n",
    "    event_concepts = []\n",
    "    for verb in verbs:\n",
    "        objects = find_possible_forms_of_objects(nouns)\n",
    "        events = link_objects_to_verb(verb, objects)\n",
    "        event_concepts.extend(events)\n",
    "    \n",
    "    return event_concepts\n",
    "\n",
    "def are_noun_phrases_similar(phrase1, phrase2):\n",
    "    nouns1 = extract_nouns(phrase1)\n",
    "    nouns2 = extract_nouns(phrase2)\n",
    "    \n",
    "    if not set(nouns1).intersection(set(nouns2)):\n",
    "        return False\n",
    "    \n",
    "    objects1 = find_possible_forms_of_objects(nouns1)\n",
    "    objects2 = find_possible_forms_of_objects(nouns2)\n",
    "    \n",
    "    M1 = set()\n",
    "    M2 = set()\n",
    "    \n",
    "    for concept in objects1:\n",
    "        M1.update(property_matches(concept))\n",
    "        \n",
    "    for concept in objects2:\n",
    "        M2.update(property_matches(concept))\n",
    "    \n",
    "    set_common = M1.intersection(M2)\n",
    "    \n",
    "    if len(set_common) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def combine_similar_events(event_concepts):\n",
    "    combined_events = []\n",
    "    used_events = set()\n",
    "    \n",
    "    for i, event1 in enumerate(event_concepts):\n",
    "        if i in used_events:\n",
    "            continue\n",
    "        similar_group = [event1]\n",
    "        for j, event2 in enumerate(event_concepts[i+1:], start=i+1):\n",
    "            if j in used_events:\n",
    "                continue\n",
    "            if are_noun_phrases_similar(event1, event2):\n",
    "                similar_group.append(event2)\n",
    "                used_events.add(j)\n",
    "        combined_events.append(similar_group)\n",
    "        used_events.add(i)\n",
    "    \n",
    "    return combined_events\n",
    "\n",
    "\n",
    "def simplify_concepts(combined_events):\n",
    "    simplified_concepts = set()\n",
    "    for event_list in combined_events:\n",
    "        for event in event_list:\n",
    "            # Split the event into words\n",
    "            words = event.split()\n",
    "            # Add non-basic action words to the simplified concepts set\n",
    "            for word in words:\n",
    "                if word not in basic_actions and word not in ['[', ']']:\n",
    "                    simplified_concepts.add(word)\n",
    "    return list(simplified_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes and keywords\n",
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}\n",
    "\n",
    "comment = retrieve_comments(1)\n",
    "\n",
    "print(\"The original comment: \", comment.iloc[0].to_list())\n",
    "\n",
    "# Example usage\n",
    "sentence = str(comment.iloc[0].to_list())\n",
    "event_concepts = extract_event_concepts(sentence)\n",
    "print(\"Extracted Event Concepts:\", event_concepts)\n",
    "\n",
    "# Combine similar events\n",
    "combined_events = combine_similar_events(event_concepts)\n",
    "print(\"Combined Events:\", combined_events)\n",
    "\n",
    "simplified_concepts = simplify_concepts(combined_events)\n",
    "print(\"Simplified Events:\", simplified_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event concept extraction algorithm\n",
    "----\n",
    "\n",
    "The algorithm for capturing event concepts matches object concepts with normalized verb chunks. This is achieved by utilizing a parse graph that maps all the multi-word expressions contained in the knowledge bases.\n",
    "1.\tMatch Object and Verb Phrases:\n",
    "\t- The algorithm searches for matches between the object concepts and the normalized verb phrases.\n",
    "2.\tUtilize a Parse Graph:\n",
    "\t- A directed, unweighted parse graph is used to quickly detect multi-word concepts without performing an exhaustive search through all possible word combinations that can form a commonsense concept.\n",
    "3.\tRemove Redundant Terms:\n",
    "\t- Single-word concepts, such as “house,” that already appear in the clause as part of a multi-word concept, like “beautiful house,” are considered pleonastic (providing redundant information) and are discarded.\n",
    "4.\tExtract Event Concepts:\n",
    "\t- The algorithm extracts event concepts such as “go market,” “buy some fruits,” “buy fruits,” and “buy vegetables.”\n",
    "\t- These event concepts represent Script-Based Object Concepts (SBoCs) and can be fed into a commonsense reasoning algorithm for further processing.\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_concept_extraction(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a given sentence.\n",
    "\n",
    "    This function processes a natural language sentence to extract event concepts by linking verbs with associated\n",
    "    noun phrases. It stems the verbs and constructs concepts by combining them with the nouns.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: extracted event concepts\n",
    "    \"\"\"\n",
    "    concepts: Set[str] = set()  # initialize an empty set to store unique concepts\n",
    "    doc = nlp(sentence)  # process the sentence with spaCy\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # identify all verbs in the sentence\n",
    "        verbs = [token for token in sent if token.pos_ == 'VERB']\n",
    "        # identify all noun phrases in the sentence\n",
    "        noun_phrases = list(sent.noun_chunks)\n",
    "\n",
    "        for verb in verbs:\n",
    "            # stem the verb\n",
    "            stemmed_verb = verb.lemma_\n",
    "            # find noun phrases associated with the verb\n",
    "            associated_nouns = [np for np in noun_phrases if np.root.head == verb]\n",
    "\n",
    "            for np in associated_nouns:\n",
    "                if len(np) > 1:\n",
    "                    # if the noun phrase contains more than one word, form a concept with the verb and noun phrase\n",
    "                    concept = f\"{stemmed_verb} {' '.join([token.text for token in np])}\"\n",
    "                    concepts.add(concept)\n",
    "                else:\n",
    "                    # handle single-word noun phrases\n",
    "                    single_word_concept = np.text\n",
    "                    if not any(single_word_concept in concept for concept in concepts):\n",
    "                        concept = f\"{stemmed_verb} {single_word_concept}\"\n",
    "                        concepts.add(concept)\n",
    "\n",
    "        for np in noun_phrases:\n",
    "            # handle noun phrases associated with auxiliary verbs\n",
    "            if np.root.head.pos_ == 'AUX':\n",
    "                concept = f\"be {' '.join([token.text for token in np])}\"\n",
    "                concepts.add(concept)\n",
    "\n",
    "    return list(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comment:\n",
      "\n",
      "\"Too many scoring options  make this game unfit for casual play.\"\n",
      "\n",
      "Extracted concepts:  ['make Too many scoring options']\n"
     ]
    }
   ],
   "source": [
    "sentence = str(retrieve_comments(12).iloc[0, 0])\n",
    "print(\"Original comment:\\n\")\n",
    "print(f\"\"\"\\\"{sentence}\\\"\"\"\")\n",
    "concepts = event_concept_extraction(sentence)\n",
    "print(\"\\nExtracted concepts: \", concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_concepts(concepts: List[str], class_keywords: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Classifies extracted concepts into predefined categories based on keywords.\n",
    "\n",
    "    Args:\n",
    "        concepts (List[str]): The list of extracted concepts.\n",
    "        class_keywords (Dict[str, List[str]]): A dictionary where keys are class names and values are lists of keywords.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Set[str]]: A dictionary where keys are class names and values are sets of concepts that match the keywords.\n",
    "    \"\"\"\n",
    "    classified_concepts: Dict[str, Set[str]] = {category: set() for category in class_keywords}\n",
    "\n",
    "    for concept in concepts:\n",
    "        for category, keywords in class_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in concept:\n",
    "                    classified_concepts[category].add(concept)\n",
    "\n",
    "    return classified_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified concepts:  {'luck or alea': set(), 'bookkeeping': set(), 'downtime': set(), 'interaction': set(), 'bash the leader': set(), 'complicated': set(), 'complex': set()}\n"
     ]
    }
   ],
   "source": [
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}\n",
    "\n",
    "classified_concepts = classify_concepts(concepts, class_keywords)\n",
    "print(\"\\nClassified concepts: \", classified_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
