{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import xmltodict\n",
    "\n",
    "from typing import List, Set, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lukas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure that necessary nltk resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_name(names):\n",
    "    if isinstance(names, list):\n",
    "        for name in names:\n",
    "            if name.get('@type') == 'primary':\n",
    "                return name.get('@value')\n",
    "    elif isinstance(names, dict) and names.get('@type') == 'primary':\n",
    "        return names.get('@value')\n",
    "    return None\n",
    "\n",
    "def retrieve_comments(index):\n",
    "    URL = \"https://boardgamegeek.com/xmlapi2/thing?id=\" + str(index) + \"&type=boardgame&comments=1\"\n",
    "    response = requests.get(URL)\n",
    "    data = xmltodict.parse(response.content)\n",
    "    game_name = get_primary_name(data[\"items\"][\"item\"]['name'])\n",
    "\n",
    "    comments = []\n",
    "    if 'items' in data and 'item' in data['items']:\n",
    "        item = data['items']['item']\n",
    "        if 'comments' in item and 'comment' in item['comments']:\n",
    "            for comment in item['comments']['comment']:\n",
    "                comments.append({game_name: comment['@value']})\n",
    "\n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event concept extraction algorithm\n",
    "----\n",
    "\n",
    "The algorithm for capturing event concepts matches object concepts with normalized verb chunks. This is achieved by utilizing a parse graph that maps all the multi-word expressions contained in the knowledge bases.\n",
    "1.\tMatch Object and Verb Phrases:\n",
    "\t- The algorithm searches for matches between the object concepts and the normalized verb phrases.\n",
    "2.\tUtilize a Parse Graph:\n",
    "\t- A directed, unweighted parse graph is used to quickly detect multi-word concepts without performing an exhaustive search through all possible word combinations that can form a commonsense concept.\n",
    "3.\tRemove Redundant Terms:\n",
    "\t- Single-word concepts, such as “house,” that already appear in the clause as part of a multi-word concept, like “beautiful house,” are considered pleonastic (providing redundant information) and are discarded.\n",
    "4.\tExtract Event Concepts:\n",
    "\t- The algorithm extracts event concepts such as “go market,” “buy some fruits,” “buy fruits,” and “buy vegetables.”\n",
    "\t- These event concepts represent Script-Based Object Concepts (SBoCs) and can be fed into a commonsense reasoning algorithm for further processing.\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_concept_extraction(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts event concepts from a given sentence.\n",
    "\n",
    "    This function processes a natural language sentence to extract event concepts by linking verbs with associated\n",
    "    noun phrases. It stems the verbs and constructs concepts by combining them with the nouns and adjectives.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to process\n",
    "\n",
    "    Returns:\n",
    "        List[str]: extracted event concepts\n",
    "    \"\"\"\n",
    "    concepts: Set[str] = set()  # initialize an empty set to store unique concepts\n",
    "    doc = nlp(sentence)  # process the sentence with spaCy\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # identify all verbs in the sentence\n",
    "        verbs = [token for token in sent if token.pos_ == 'VERB']\n",
    "        # identify all noun phrases in the sentence\n",
    "        noun_phrases = list(sent.noun_chunks)\n",
    "\n",
    "        for verb in verbs:\n",
    "            # stem the verb\n",
    "            stemmed_verb = verb.lemma_\n",
    "            # find noun phrases associated with the verb\n",
    "            associated_nouns = [np for np in noun_phrases if np.root.head == verb]\n",
    "\n",
    "            for np in associated_nouns:\n",
    "                # extract adjectives in the noun phrase\n",
    "                adjectives = [token.text for token in np if token.pos_ == 'ADJ']\n",
    "                if len(np) > 1:\n",
    "                    # if the noun phrase contains more than one word, form a concept with the verb and noun phrase\n",
    "                    concept = f\"{stemmed_verb} {' '.join([token.text for token in np])}\"\n",
    "                    if adjectives:\n",
    "                        concept += f\" {' '.join(adjectives)}\"\n",
    "                    concepts.add(concept)\n",
    "                else:\n",
    "                    # handle single-word noun phrases\n",
    "                    single_word_concept = np.text\n",
    "                    if not any(single_word_concept in concept for concept in concepts):\n",
    "                        concept = f\"{stemmed_verb} {single_word_concept}\"\n",
    "                        if adjectives:\n",
    "                            concept += f\" {' '.join(adjectives)}\"\n",
    "                        concepts.add(concept)\n",
    "\n",
    "        for np in noun_phrases:\n",
    "            # handle noun phrases associated with auxiliary verbs\n",
    "            if np.root.head.pos_ == 'AUX':\n",
    "                adjectives = [token.text for token in np if token.pos_ == 'ADJ']\n",
    "                concept = f\"be {' '.join([token.text for token in np])}\"\n",
    "                if adjectives:\n",
    "                    concept += f\" {' '.join(adjectives)}\"\n",
    "                concepts.add(concept)\n",
    "\n",
    "    return list(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comment:\n",
      "\n",
      "\"What an elegant game.   Light rules, deep gameplay, language independent, attractive presentation. Do they still make games like this?  Some say it's abstract but to me it looks thematic. Castes, tiles, board and screen all make sense. Abstract is what Ingenious looks like; Samurai is not that abstract.   Rules are clear and concise. Knizia usually puts me off with intricate end game conditions but here the scoring is pretty logical. It's simple as that: you aim to either get a majority in two castes, or get a majority in one caste while staying competitive in the other two castes. In other terms: you need two big scores (AB) or one big score and two small ones (Abc).  There is a slight difference between editions: in the original if no player has majority in a caste, everyone loses. In the zman edition the player with most pieces of all castes wins.  First player advantage is overrated and is of no significance if players draw starting tiles randomly.\"\n",
      "\n",
      "Extracted concepts:  ['look it', 'aim you', 'overrate First player advantage First', 'be Rules', 'be it', 'be It', 'be the scoring', 'make they', 'put Knizia', 'make games', 'start tiles', 'make Castes', 'say Some', 'be a slight difference slight', 'have no player', 'get a majority', 'draw players', 'lose everyone', 'need two big scores big', 'make sense', 'be Samurai']\n"
     ]
    }
   ],
   "source": [
    "sentence = str(retrieve_comments(3).iloc[0, 0])\n",
    "print(\"Original comment:\\n\")\n",
    "print(f\"\"\"\\\"{sentence}\\\"\"\"\")\n",
    "concepts = event_concept_extraction(sentence)\n",
    "print(\"\\nExtracted concepts: \", concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple matching for classification\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified classes\n",
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_concepts(concepts: List[str], class_keywords: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Classifies extracted concepts into predefined categories based on keywords.\n",
    "\n",
    "    Args:\n",
    "        concepts (List[str]): The list of extracted concepts.\n",
    "        class_keywords (Dict[str, List[str]]): A dictionary where keys are class names and values are lists of keywords.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Set[str]]: A dictionary where keys are class names and values are sets of concepts that match the keywords.\n",
    "    \"\"\"\n",
    "    classified_concepts: Dict[str, Set[str]] = {category: set() for category in class_keywords}\n",
    "\n",
    "    for concept in concepts:\n",
    "        for category, keywords in class_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in concept:\n",
    "                    classified_concepts[category].add(concept)\n",
    "\n",
    "    return classified_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified concepts:  {'luck or alea': set(), 'bookkeeping': set(), 'downtime': set(), 'interaction': set(), 'bash the leader': set(), 'complicated': set(), 'complex': set()}\n"
     ]
    }
   ],
   "source": [
    "class_keywords = {\n",
    "    'luck or alea': ['luck', 'chance', 'alea'],\n",
    "    'bookkeeping': ['bookkeeping', 'recording', 'rulebook'],\n",
    "    'downtime': ['downtime', 'waiting'],\n",
    "    'interaction': ['interaction', 'influence'],\n",
    "    'bash the leader': ['bash the leader', 'sacrifice'],\n",
    "    'complicated': ['complicated', 'rules', 'exceptions'],\n",
    "    'complex': ['complex', 'repercussions', 'unpredictable'],\n",
    "}\n",
    "\n",
    "classified_concepts = classify_concepts(concepts, class_keywords)\n",
    "print(\"\\nClassified concepts: \", classified_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use of similarity meassure\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ')\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=float)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def get_combined_embedding(words, embeddings):\n",
    "    valid_embeddings = [embeddings.get(word) for word in words if word in embeddings]\n",
    "    if not valid_embeddings:\n",
    "        return None\n",
    "    return np.mean(valid_embeddings, axis=0)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_similarity(concept, class_keywords, embeddings, similarity_threshold=0.5, top_n_classes=3):\n",
    "    concept_words = concept.split()\n",
    "    concept_embedding = get_combined_embedding(concept_words, embeddings)\n",
    "    if concept_embedding is None:\n",
    "        return None, None\n",
    "    \n",
    "    similarities = []\n",
    "    for class_name, keywords in class_keywords.items():\n",
    "        class_embedding = get_combined_embedding(keywords, embeddings)\n",
    "        if class_embedding is None:\n",
    "            continue\n",
    "        similarity = cosine_similarity(concept_embedding, class_embedding)\n",
    "        if similarity is not None:\n",
    "            similarities.append((class_name, similarity))\n",
    "    \n",
    "    # Filter classes based on similarity threshold\n",
    "    similarities = [item for item in similarities if item[1] >= similarity_threshold]\n",
    "    # Sort by similarity and get top N\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_classes = similarities[:top_n_classes]\n",
    "    \n",
    "    if not top_classes:\n",
    "        return None, None\n",
    "            \n",
    "    return top_classes[0]  # Return the best class and its similarity\n",
    "\n",
    "# load the embeddings\n",
    "embeddings = load_embeddings('./data/numberbatch-en.txt')\n",
    "\n",
    "# compute the similarity\n",
    "assignments = {}\n",
    "for concept in concepts:\n",
    "    best_class, similarity = compute_similarity(concept, class_keywords, embeddings, similarity_threshold=0.1, top_n_classes=1)\n",
    "    assignments[concept] = (best_class, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignments:\n",
      "Concept 'look it' could not be assigned to any class\n",
      "Concept 'aim you' is assigned to class 'luck or alea' with similarity 0.15\n",
      "Concept 'overrate First player advantage First' is assigned to class 'luck or alea' with similarity 0.22\n",
      "Concept 'be Rules' could not be assigned to any class\n",
      "Concept 'be it' could not be assigned to any class\n",
      "Concept 'be It' could not be assigned to any class\n",
      "Concept 'be the scoring' is assigned to class 'bookkeeping' with similarity 0.17\n",
      "Concept 'make they' is assigned to class 'bash the leader' with similarity 0.15\n",
      "Concept 'put Knizia' is assigned to class 'bash the leader' with similarity 0.14\n",
      "Concept 'make games' is assigned to class 'complicated' with similarity 0.18\n",
      "Concept 'start tiles' could not be assigned to any class\n",
      "Concept 'make Castes' is assigned to class 'bash the leader' with similarity 0.19\n",
      "Concept 'say Some' could not be assigned to any class\n",
      "Concept 'be a slight difference slight' is assigned to class 'complicated' with similarity 0.18\n",
      "Concept 'have no player' is assigned to class 'complicated' with similarity 0.21\n",
      "Concept 'get a majority' is assigned to class 'luck or alea' with similarity 0.12\n",
      "Concept 'draw players' is assigned to class 'luck or alea' with similarity 0.16\n",
      "Concept 'lose everyone' is assigned to class 'bash the leader' with similarity 0.15\n",
      "Concept 'need two big scores big' is assigned to class 'complicated' with similarity 0.18\n",
      "Concept 'make sense' is assigned to class 'bash the leader' with similarity 0.20\n",
      "Concept 'be Samurai' could not be assigned to any class\n"
     ]
    }
   ],
   "source": [
    "# ouput of the resutls\n",
    "print(\"Assignments:\")\n",
    "for concept, best_class in assignments.items():\n",
    "    if best_class != (None, None):\n",
    "        class_name, similarity = best_class\n",
    "        print(f\"Concept '{concept}' is assigned to class '{class_name}' with similarity {similarity:.2f}\")\n",
    "    else:\n",
    "        print(f\"Concept '{concept}' could not be assigned to any class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
